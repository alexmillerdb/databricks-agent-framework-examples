# DSPy RAG Agent Configuration
# This file contains all parameterized configurations for the agent, 
# vector search, and LLM settings

# LLM Configuration
# Default LLM configuration (fallback for any component without specific config)
llm_config:
  endpoint: "databricks/databricks-meta-llama-3-1-8b-instruct"
  max_tokens: 2500
  temperature: 0.01
  top_p: 0.95
  
# Component-specific LLM configurations (optional - falls back to llm_config if not specified)
llm_endpoints:
  # LLM for query rewriting (can use a smaller, faster model)
  query_rewriter:
    endpoint: "databricks/databricks-meta-llama-3-1-8b-instruct"
    max_tokens: 150
    temperature: 0.3
    top_p: 0.95
  
  # LLM for RAG response generation (main model)
  response_generator:
    endpoint: "databricks/databricks-meta-llama-3-3-70b-instruct"
    max_tokens: 2500
    temperature: 0.01
    top_p: 0.95
  
  # LLM for optimization evaluation judges
  optimization_judge:
    endpoint: "databricks/databricks-claude-3-7-sonnet"
    max_tokens: 1000
    temperature: 0.0
    top_p: 0.95

# Vector Search Configuration
vector_search:
  index_fullname: users.alex_miller.wikipedia_chunks_index  # or can be set via environment variable VS_INDEX_FULLNAME
  text_column_name: "chunk"
  docs_id_column_name: "id"
  columns: ["id", "title", "chunk_id"]
  top_k: 5

# DSPy Configuration
dspy_config:
  response_generator_signature: "context, request -> response"
  optimized_program_path: "optimized_rag_program.json"  # Will be set via environment variable DSPY_OPTIMIZED_PROGRAM_PATH
  
# Agent Configuration
agent_config:
  use_optimized: true
  use_query_rewriter: true  # Enable query rewriting for better retrieval
  enable_tracing: true
  verbose: true
  max_iterations: 10

# MLflow Configuration
mlflow_config:
  enable_autolog: true
  experiment_name: "dspy-rag-agent"
  registered_model_name: "dspy_rag_chat_agent"